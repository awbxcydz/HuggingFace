{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Code Generation**\n",
        "We shall build a scaled-down version of a code generation model using GPT-2 with the help of TensorFlow. We shall train the tokenizer and the model from scratch using a subset of CodeParrot dataset."
      ],
      "metadata": {
        "id": "ujyXkEBsD8pt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Install and Import Required Libraries**"
      ],
      "metadata": {
        "id": "WfyFBHbiGOd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers"
      ],
      "metadata": {
        "id": "zJPSWybSGRiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "from transformers import AutoTokenizer, PreTrainedTokenizerFast, AutoConfig, TFGPT2LMHeadModel, DataCollatorForLanguageModeling, create_optimizer, pipeline\n",
        "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers, processors, decoders\n",
        "from datasets import load_dataset, Dataset"
      ],
      "metadata": {
        "id": "BmjBcE_yORq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Load Data**"
      ],
      "metadata": {
        "id": "cT9ixgPlc25u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset = load_dataset('codeparrot/codeparrot-valid-near-deduplication', split='train', streaming=True)"
      ],
      "metadata": {
        "id": "DUa9llZBOSJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Preprocess Data**"
      ],
      "metadata": {
        "id": "KHAj9LBkc910"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def any_keyword_in_string(string, keywords):\n",
        "  for keyword in keywords:\n",
        "    if keyword in string:\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def filter_streaming_dataset(dataset, filters):\n",
        "  filtered_dict = collections.defaultdict(list)\n",
        "  total = 0\n",
        "  for sample in iter(dataset):\n",
        "    total += 1\n",
        "    if any_keyword_in_string(sample['content'], filters):\n",
        "      for key, value in sample.items():\n",
        "        filtered_dict[key].append(value)\n",
        "  print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n",
        "  return Dataset.from_dict(filtered_dict)"
      ],
      "metadata": {
        "id": "5X7Lz5dmOSGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filters = ['pandas', 'sklearn', 'matplotlib', 'seaborn']\n",
        "filtered_dataset = filter_streaming_dataset(raw_dataset, filters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emo3-DrLSYck",
        "outputId": "bab6451a-5df4-4455-e3c9-59a604c014c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.18% of data after filtering.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_dataset = filtered_dataset.train_test_split(train_size=1000, test_size=100, seed=44)\n",
        "filtered_dataset['validation'] = filtered_dataset.pop('test')\n",
        "filtered_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ6IZBiUTCtH",
        "outputId": "2bf2712f-f150-4c63-bb56-6b48e09abfa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license', 'hash', 'line_mean', 'line_max', 'alpha_frac', 'autogenerated'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license', 'hash', 'line_mean', 'line_max', 'alpha_frac', 'autogenerated'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Build and Train a New Tokenizer**\n",
        "### Train a New Tokenizer from Existing Tokenizer"
      ],
      "metadata": {
        "id": "nkdlS1j9fUle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_corpus(example):\n",
        "  return (example['train'][i : i+1000]['content'] for i in range(0, len(example['train']), 1000))"
      ],
      "metadata": {
        "id": "VlDGSXJpTDat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "old_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "new_tokenizer = old_tokenizer.train_new_from_iterator(get_training_corpus(filtered_dataset), vocab_size=52000)"
      ],
      "metadata": {
        "id": "PaupvXe6TDRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = '''def add_numbers(a, b):\n",
        "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
        "    return a + b'''\n",
        "\n",
        "print(old_tokenizer.tokenize(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bILvkmfMaAoI",
        "outputId": "04a9c596-7982-43a6-81a0-c3dca40ba73f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['def', 'Ġadd', '_', 'n', 'umbers', '(', 'a', ',', 'Ġb', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`', '.\"', '\"\"', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_tokenizer.tokenize(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxmuiSkhaAbJ",
        "outputId": "04a46761-22dc-47be-99ff-6e2afc698bd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['def', 'Ġadd', '_', 'numbers', '(', 'a', ',', 'Ġb', '):', 'ĊĠĠĠ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`.\"\"\"', 'ĊĠĠĠ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build a New Tokenizer from Scratch"
      ],
      "metadata": {
        "id": "3-em-PyLYNFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(models.BPE())"
      ],
      "metadata": {
        "id": "jTMDHs7bVWt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-2 doesn't use normalizer and hence, skipping it\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
        "\n",
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axoC6zbUVWrM",
        "outputId": "ab713fc0-e47a-44c8-c904-055b5d97f90b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Let', (0, 3)),\n",
              " (\"'s\", (3, 5)),\n",
              " ('Ġtest', (5, 10)),\n",
              " ('Ġpre', (10, 14)),\n",
              " ('-', (14, 15)),\n",
              " ('tokenization', (15, 27)),\n",
              " ('!', (27, 28))]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
        "tokenizer.train_from_iterator(get_training_corpus(filtered_dataset), trainer=trainer)\n",
        "\n",
        "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2Niln5YVWlF",
        "outputId": "0a334057-be6a-46fe-adb7-15fb81c95422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['L', 'et', \"'s\", 'Ġtest', 'Ġthis', 'Ġtokenizer', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
        "\n",
        "sentence = \"Let's test this tokenizer.\"\n",
        "encoding = tokenizer.encode(sentence)\n",
        "start, end = encoding.offsets[4]\n",
        "sentence[start : end]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zxGxxW0hhk86",
        "outputId": "8044d06e-450f-438c-c55f-e7f167da9535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' this'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decoder = decoders.ByteLevel()\n",
        "\n",
        "tokenizer.decode(encoding.ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7xkTfOqTizfg",
        "outputId": "56b366ce-cfb0-409f-da4f-f5bb171c91cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Let's test this tokenizer.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    bos_token=\"<|endoftext|>\",\n",
        "    eos_token=\"<|endoftext|>\"\n",
        ")\n",
        "\n",
        "tokenizer = wrapped_tokenizer"
      ],
      "metadata": {
        "id": "ItWr4_9rjR6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = 128\n",
        "\n",
        "def tokenizer_function(example):\n",
        "  outputs = tokenizer(\n",
        "      example['content'],\n",
        "      max_length=context_length,\n",
        "      truncation=True,\n",
        "      return_overflowing_tokens=True,\n",
        "      return_length=True\n",
        "  )\n",
        "\n",
        "  # Discarding chunks with length not equal to context_length\n",
        "  input_batch = list()\n",
        "  for length, input_ids in zip(outputs['length'], outputs['input_ids']):\n",
        "    if length == context_length:\n",
        "      input_batch.append(input_ids)\n",
        "\n",
        "  return {'input_ids': input_batch}"
      ],
      "metadata": {
        "id": "XuHKTEkWjR3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = filtered_dataset.map(tokenizer_function, batched=True, remove_columns=filtered_dataset['train'].column_names)"
      ],
      "metadata": {
        "id": "MuTgSlxYizch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL-1JflKdp2m",
        "outputId": "648b54fb-4ee8-405a-ed11-ad04898916e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 23839\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 2709\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Initialize a New Model and Train from Scratch**"
      ],
      "metadata": {
        "id": "knVZLgRMeVcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained(\n",
        "    'gpt2',\n",
        "    vocab_size=len(tokenizer),\n",
        "    n_ctx=context_length,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")"
      ],
      "metadata": {
        "id": "yizJaiu8ePHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFGPT2LMHeadModel(config)\n",
        "model(model.dummy_inputs) # Building the model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMynI_5veO8K",
        "outputId": "f03b3147-5275-4c39-8cba-031617f5d74f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tfgpt2lm_head_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " transformer (TFGPT2MainLaye  multiple                 105042432 \n",
            " r)                                                              \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 105,042,432\n",
            "Trainable params: 105,042,432\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors='tf')\n",
        "\n",
        "tf_train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_dataset['train'],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "tf_validation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_dataset['validation'],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=False,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "zv9Wzt88oYgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "num_train_steps = len(tf_train_dataset) * num_epochs\n",
        "\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=5e-5,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=1000,\n",
        "    weight_decay_rate=0.01\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "wcT5XGKKpkVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training in mixed-precision float16\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "history = model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=num_epochs, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fu1ACl70pleQ",
        "outputId": "426a562e-3615-48ef-bf05-5350ef060d2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "744/744 [==============================] - 922s 1s/step - loss: 7.3479 - accuracy: 0.0539 - val_loss: 5.9111 - val_accuracy: 0.0126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Predict using the Trained Model**"
      ],
      "metadata": {
        "id": "ZoTv2RkJtd1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code_generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0)"
      ],
      "metadata": {
        "id": "Nv3h1gukplbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"\"\"\\\n",
        "# create some data\n",
        "x = np.random.randn(100)\n",
        "y = np.random.randn(100)\n",
        "\n",
        "# create dataframe from x and y\n",
        "\"\"\"\n",
        "\n",
        "# Model needs to be retrained with additional data for improving accuracy\n",
        "print(code_generator(txt, num_return_sequences=1, pad_token_id=code_generator.tokenizer.eos_token_id)[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j367co8nfGES",
        "outputId": "dda89520-0b41-495d-bde3-b161f6811d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# create some data\n",
            "x = np.random.randn(100)\n",
            "y = np.random.randn(100)\n",
            "\n",
            "# create dataframe from x and y\n",
            "def test_file_file1]\n",
            "def return y=0\n"
          ]
        }
      ]
    }
  ]
}