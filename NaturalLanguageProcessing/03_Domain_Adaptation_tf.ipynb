{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Domain Adaptation**\n",
        "Domain adaptation is the process of fine-tuning a pretrained language model on in-domain data.\n",
        "\n",
        "We shall fine-tune a DistilBERT model using TensorFlow on IMDB dataset. After fine-tuning, the model should adapt its vocabulary from the factual data of Wikipedia that it was pretrained on to the more subjective elements of movie reviews."
      ],
      "metadata": {
        "id": "n8do0I3gWJb5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Install and Import Required Libraries**"
      ],
      "metadata": {
        "id": "Vj1eLaITWJRx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywSnL2M10uaw"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import collections\n",
        "import math\n",
        "\n",
        "from transformers import AutoTokenizer, TFAutoModelForMaskedLM, DataCollatorForLanguageModeling, create_optimizer, pipeline\n",
        "from transformers.data.data_collator import tf_default_data_collator\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "FKEQe66dWH7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Load Data**"
      ],
      "metadata": {
        "id": "g-eOugQueSIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset = load_dataset('imdb')"
      ],
      "metadata": {
        "id": "8C4plPLXWHh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJlF1J1VWHcc",
        "outputId": "d4075f54-dd35-41a8-b214-6e656afc0bae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    unsupervised: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Preprocess Data**"
      ],
      "metadata": {
        "id": "Fut9Vx4Ie4in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = 'distilbert-base-uncased'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "_zpHSo1RLF9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "  result = tokenizer(examples['text'])\n",
        "\n",
        "  if tokenizer.is_fast:\n",
        "    result['word_ids'] = [result.word_ids(i) for i, _ in enumerate(result['input_ids'])]\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "kxlac8qSfyz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True, remove_columns=['text', 'label'])"
      ],
      "metadata": {
        "id": "em7UDuoziXfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssv9nqvfn-0_",
        "outputId": "c505886a-d410-49d6-b5b9-90a1b35a38a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    unsupervised: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenating all texts and splitting into chunks\n",
        "chunk_size = 128\n",
        "\n",
        "def group_texts(examples):\n",
        "  # Concatenating all texts\n",
        "  concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "\n",
        "  # Computing length of concatenated texts\n",
        "  total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "  # Dropping last chunk if it is smaller than chunk_size\n",
        "  total_length = (total_length // chunk_size) * chunk_size\n",
        "  # Splitting into chunks\n",
        "  result = {key: [value[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
        "            for key, value in concatenated_examples.items()}\n",
        "\n",
        "  # Copying input_ids and creating labels before randomly masking input_ids\n",
        "  result['labels'] = result['input_ids'].copy()\n",
        "  return result"
      ],
      "metadata": {
        "id": "0wXZfRGkhjGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm_dataset = tokenized_dataset.map(group_texts, batched=True)"
      ],
      "metadata": {
        "id": "-CKuRbwdhjDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwTqA-GEWm4i",
        "outputId": "6ddbbf7b-8f96-4bd7-a87d-b88be4756307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
              "        num_rows: 61291\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
              "        num_rows: 59904\n",
              "    })\n",
              "    unsupervised: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
              "        num_rows: 122957\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "downsampled_dataset = lm_dataset['train'].train_test_split(train_size=11000, test_size=1000, seed=44)\n",
        "test_dataset = downsampled_dataset.pop('test')\n",
        "\n",
        "downsampled_dataset = downsampled_dataset['train'].train_test_split(train_size=10000, test_size=1000, seed=44)\n",
        "downsampled_dataset['validation'] = downsampled_dataset.pop('test')\n",
        "downsampled_dataset['test'] = test_dataset"
      ],
      "metadata": {
        "id": "kIEhPoksoa8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "downsampled_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXsIyDCoobn2",
        "outputId": "59b70313-293e-4cfb-d557-518ada19335d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
        "\n",
        "tf_train_dataset = model.prepare_tf_dataset(\n",
        "    downsampled_dataset['train'],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "tf_validation_dataset = model.prepare_tf_dataset(\n",
        "    downsampled_dataset['validation'],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=False,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "tf_test_dataset = model.prepare_tf_dataset(\n",
        "    downsampled_dataset['test'],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=False,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "UCFX6zdeKCJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator for whole word masking\n",
        "wwm_probability = 0.2\n",
        "\n",
        "def whole_word_masking_data_collator(features):\n",
        "  for feature in features:\n",
        "    word_ids = feature.pop('word_ids')\n",
        "    mapping_dict = collections.defaultdict(list)\n",
        "    current_word_index = -1\n",
        "    current_word = None\n",
        "\n",
        "    for i, word_id in enumerate(word_ids):\n",
        "      if word_id is not None:\n",
        "        if word_id != current_word:\n",
        "          current_word = word_id\n",
        "          current_word_index += 1\n",
        "        mapping_dict[current_word_index].append(i)\n",
        "\n",
        "    mask = np.random.binomial(1, wwm_probability, len(mapping_dict))\n",
        "\n",
        "    input_ids = feature['input_ids']\n",
        "    labels = feature['labels']\n",
        "    new_labels = [-100] * len(labels)\n",
        "\n",
        "    for word_id in np.where(mask)[0]:\n",
        "      for i in mapping_dict[word_id]:\n",
        "        new_labels[i] = labels[i] # Setting values to the ones corresponding to masked words and rest are set to -100\n",
        "        input_ids[i] = tokenizer.mask_token_id\n",
        "    feature['labels'] = new_labels\n",
        "\n",
        "  return tf_default_data_collator(features)"
      ],
      "metadata": {
        "id": "7CqrXHMgrQY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = [lm_dataset['train'][i] for i in range(2)]\n",
        "batch = whole_word_masking_data_collator(samples)\n",
        "\n",
        "for chunk in batch['input_ids']:\n",
        "  print(f\"{tokenizer.decode(chunk)}\")"
      ],
      "metadata": {
        "id": "I-KVl3oJr2J1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "963eb84a-1fdc-4a01-c025-4c681225e1de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] i rented i am curious - yellow from [MASK] video store because of all the controversy that surrounded [MASK] when [MASK] [MASK] first released in 1967 [MASK] [MASK] also heard that at first it was seized by u [MASK] s. customs if it ever tried to enter this [MASK], therefore being a fan of [MASK] considered \" [MASK] \" i really had [MASK] see this for [MASK]. < [MASK] / > < br / > [MASK] plot is [MASK] around a young [MASK] drama student named lena who wants to learn everything she can [MASK] life. in particular she wants to [MASK] her [MASK] [MASK] to making [MASK] sort of documentary on what the average swede thought about certain political issues such\n",
            "as the [MASK] war [MASK] race issues in the [MASK] states. in between asking politicians and ordinary denizens of stockholm [MASK] their opinions on politics, she has sex with [MASK] drama teacher, classmates, and married men. < br / > < br [MASK] > [MASK] kills me about i am curious [MASK] yellow is that 40 years ago [MASK] this was [MASK] pornographic [MASK] really [MASK] the sex and nudity [MASK] [MASK] [MASK] and far between, even then it's not shot like some cheaply made porno. while [MASK] countrymen mind find it [MASK] [MASK] in reality sex and nudity [MASK] a major staple in swedish cinema. even ingmar bergman,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Predict using the Model before Fine-tuning**"
      ],
      "metadata": {
        "id": "6ZqqEQm4LN54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a great [MASK].\""
      ],
      "metadata": {
        "id": "KnzK_wuzLP4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = tokenizer(text, return_tensors='np')\n",
        "logits = model(**tokenized_text)['logits']"
      ],
      "metadata": {
        "id": "XqfqN9wrLPxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding location of [MASK] and extracting its logits\n",
        "mask_token_index = np.argwhere(tokenized_text['input_ids'] == tokenizer.mask_token_id)[0, 1]\n",
        "mask_token_logits = logits[0, mask_token_index, :]\n",
        "\n",
        "# Picking the [MASK] candidates with the highest logits\n",
        "# Negating the array before argsort to get the largest, not the smallest, logits\n",
        "top_5_tokens = np.argsort(-mask_token_logits)[:5].tolist()"
      ],
      "metadata": {
        "id": "mVq-xWCLLPrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in top_5_tokens:\n",
        "  print(f\"{text.replace(tokenizer.mask_token, tokenizer.decode(token))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KW6GDIuHLPk1",
        "outputId": "56cb47cf-b3f9-4c7d-8090-5e242864ea7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a great deal.\n",
            "This is a great success.\n",
            "This is a great adventure.\n",
            "This is a great idea.\n",
            "This is a great feat.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Fine-tune the Model**"
      ],
      "metadata": {
        "id": "t2ejPLohKHx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "num_train_steps = len(tf_train_dataset) * num_epochs\n",
        "\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=2e-5,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=1000,\n",
        "    weight_decay_rate=0.01\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "m49cEQtnr2EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perplexity before fine-tuning the model\n",
        "loss, accuracy = model.evaluate(tf_test_dataset)\n",
        "print(f\"Perplexity: {math.exp(loss):.2f}\")"
      ],
      "metadata": {
        "id": "c2d7fVP4r2BR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40418421-ab6d-4cc2-e031-581690b72d01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 13s 244ms/step - loss: 3.1184 - accuracy: 0.0653\n",
            "Perplexity: 22.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training in mixed-precision float16\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "history = model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=num_epochs, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzRWbAowsb3A",
        "outputId": "8eac1b8c-d619-45a9-c4b2-721001f4e47d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "312/312 [==============================] - 204s 632ms/step - loss: 2.8610 - accuracy: 0.0713 - val_loss: 2.5994 - val_accuracy: 0.0753\n",
            "Epoch 2/5\n",
            "312/312 [==============================] - 197s 631ms/step - loss: 2.6592 - accuracy: 0.0752 - val_loss: 2.4981 - val_accuracy: 0.0796\n",
            "Epoch 3/5\n",
            "312/312 [==============================] - 199s 639ms/step - loss: 2.5653 - accuracy: 0.0770 - val_loss: 2.4410 - val_accuracy: 0.0804\n",
            "Epoch 4/5\n",
            "312/312 [==============================] - 200s 640ms/step - loss: 2.5124 - accuracy: 0.0780 - val_loss: 2.4288 - val_accuracy: 0.0805\n",
            "Epoch 5/5\n",
            "312/312 [==============================] - 197s 631ms/step - loss: 2.4648 - accuracy: 0.0792 - val_loss: 2.4172 - val_accuracy: 0.0810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perplexity after fine-tuning the model\n",
        "loss, accuracy = model.evaluate(tf_test_dataset)\n",
        "print(f\"Perplexity: {math.exp(loss):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ptA3B-isbr9",
        "outputId": "e41a3f84-bf16-47f5-e891-7fc9a2bca195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 8s 243ms/step - loss: 2.3766 - accuracy: 0.0824\n",
            "Perplexity: 10.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Predict using the Fine-tuned Model**"
      ],
      "metadata": {
        "id": "VKLHc6Q04kRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_filler = pipeline('fill-mask', model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "z4pyovW64k9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = mask_filler(text)\n",
        "\n",
        "for pred in predictions:\n",
        "  print(f\"{pred['sequence']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDTRqXPj4lYP",
        "outputId": "f6db1743-6eae-4ff8-8536-4dcfc95d9d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a great film.\n",
            "this is a great movie.\n",
            "this is a great idea.\n",
            "this is a great adventure.\n",
            "this is a great show.\n"
          ]
        }
      ]
    }
  ]
}